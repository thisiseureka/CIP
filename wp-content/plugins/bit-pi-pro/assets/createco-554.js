import{a3 as c,a4 as r}from"./_applist-952.js";import{_ as e}from"./main-red-poems-invite.js";import{s as E}from"./machine.-578.js";import{a as u}from"./machineh-459.js";import{h as _,b as l,c as h}from"./openaico-17.js";import"./lodash-840.js";import"./mutative-12.js";globalThis.jotaiAtomCache=globalThis.jotaiAtomCache||{cache:new Map,get(t,a){return this.cache.has(t)?this.cache.get(t):(this.cache.set(t,a),a)}};e("Alloy"),e("Ash"),e("Ballad"),e("Coral"),e("Echo"),e("Fable"),e("Onyx"),e("Nova"),e("Sage"),e("Shimmer"),e("Verse");e("MP3"),e("Opus"),e("AAC"),e("FLAC"),e("WAV"),e("PCM");const b=[{label:e("User"),value:"user"},{label:e("Assistant"),value:"assistant"},{label:e("Developer/System"),value:"developer"}],T=[{label:e("logit_bias"),value:"logit_bias"},{label:e("logprobs"),value:"logprobs"},{label:e("metadata"),value:"metadata"},{label:e("modalities"),value:"modalities"},{label:e("parallel_tool_calls"),value:"parallel_tool_calls"},{label:e("prediction"),value:"prediction"},{label:e("service_tier"),value:"service_tier"},{label:e("store"),value:"store"},{label:e("stream"),value:"stream"},{label:e("stream_options"),value:"stream_options"},{label:e("tool_choice"),value:"tool_choice"},{label:e("tools"),value:"tools"},{label:e("top_logprobs"),value:"top_logprobs"},{label:e("user"),value:"user"},{label:e("web_search_options"),value:"web_search_options"}];globalThis.jotaiAtomCache=globalThis.jotaiAtomCache||{cache:new Map,get(t,a){return this.cache.has(t)?this.cache.get(t):(this.cache.set(t,a),a)}};const v=["response-format","reasoning-effort","temperature","top-p","number","frequency-penalty","presence-penalty","seed"],I=c(({helpers:t})=>({states:{components:[h(t),{componentName:t.componentName.select,id:"model-id",label:e("Select Model"),onChange:"SET_MODEL",onRefetch:"REFETCH_MODEL_LIST",optionFilterProp:"label",options:[],path:"model",placeholder:e("Select a Model"),render:"IF_CONNECTION_SELECTED",required:!0,showSearch:!0,value:""},{componentName:t.componentName.mixInput,helperText:e("If empty, the limit of the model will be used. Note that: - Low values may cause the output to be truncated. - High values may use a lot of OpenAI credit. When using reasoning models such as the o1 models, this value is the sum of reasoning + output tokens."),id:"max-token",label:e("Max Completion Token"),onChange:"SET_MAX_TOKEN",path:"max_tokens",render:"IF_MODEL_SELECTED",required:!0,value:[]},{addItemButtonLabel:e("Add Message"),componentName:t.componentName.repeaterField,direction:"vertical",fieldsMetaData:[{componentName:t.componentName.select,id:"messages",label:e("Field"),name:"role",options:b,required:!0,value:""},{componentName:t.componentName.mixInput,label:e("Value"),name:"content",required:!0,value:[],wrapperClassName:"w-100"}],id:"messages-list",inputGroupProps:{label:e("Role #{COUNT}")},label:e("Messages"),onChange:"ON_FIELD_MAP_CHANGE",render:"IF_MODEL_SELECTED",value:[]},{componentName:t.componentName.switch,id:"show-advance-feature",label:e("Show Advance Feature"),onChange:"SET_FEATURE",path:"advance-feature",render:"IF_MODEL_SELECTED",value:!1},{componentName:t.componentName.select,helperText:e('When using JSON Object, you must also instruct the model to produce JSON via a System or User message. If not, the model can generate an unending stream of whitespace until it reaches the token limit. This will result in a long-running and seemingly "stuck" request.'),id:"response-format",label:e("Response Format"),onChange:"SET_RESPONSE",options:[{label:e("Text"),value:"text"},{label:e("JSON Object"),value:"object"}],path:"response_format",placeholder:e("Select a Format"),render:"IF_ADVANCE_FEATURE_SELECTED",required:!0,value:"text"},{componentName:t.componentName.select,helperText:e("Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."),id:"reasoning_effort",label:e("Reasoning Effort"),onChange:"SET_REASONING_EFFECT",options:[{label:e("Low"),value:"low"},{label:e("Medium"),value:"medium"},{label:e("High"),value:"high"}],path:"reasoning_effort",placeholder:e("Select a Effort"),render:"IF_REASONING_MODEL_SELECTED",required:!0,value:void 0},{componentName:t.componentName.mixInput,helperText:e("Higher temperatures generate more diverse and creative responses. For example, 0.8. Lower temperatures generate more focused and well-defined responses. For example, 0.2. The default value is 1. Must be lower than or equal to 2."),id:"temperature",label:e("Temperature"),onChange:"SET_TEMPERATURE",path:"temperature",render:"IF_ADVANCE_FEATURE_SELECTED",value:[]},{componentName:t.componentName.mixInput,helperText:e("Alternative to sampling with temperature, based on token probability. For example, .1 means only the tokens in the top 10% probability mass are considered. The default value is 1. Must be lower than or equal to 1."),id:"top-p",label:e("Top P"),onChange:"SET_TOP_P",path:"top_p",render:"IF_ADVANCE_FEATURE_SELECTED",value:[]},{componentName:t.componentName.mixInput,helperText:e("Number of responses to generate. The results can be found in the modules output in Choices. The default value is 1."),id:"number",label:e("Number"),onChange:"SET_NUMBER",path:"n",render:"IF_ADVANCE_FEATURE_SELECTED",value:[]},{componentName:t.componentName.mixInput,helperText:e("Number of responses to generate. The results can be found in the modules output in Choices. The default value is 1."),id:"frequency-penalty",label:e("Frequency Penalty"),onChange:"SET_FREQUENCY_PENALTY",path:"frequency_penalty",render:"IF_ADVANCE_FEATURE_SELECTED",value:[]},{componentName:t.componentName.mixInput,helperText:e("Positive values discourage repeated token usage, increasing diversity and the likelihood of new topics. Negative values encourage repeated token usage, reinforcing existing patterns. Must be a number between -2 and 2."),id:"presence-penalty",label:e("Presence Penalty"),onChange:"SET_PRESENCE_PENALTY",path:"presence_penalty",render:"IF_ADVANCE_FEATURE_SELECTED",value:[]},{componentName:t.componentName.mixInput,helperText:e("This feature is in Beta. If specified, OpenAI will make a best effort to sample deterministically so that repeated requests with the same seed and parameters return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend."),id:"seed",label:e("Seed"),onChange:"SET_SEED",path:"seed",render:"IF_ADVANCE_FEATURE_SELECTED",value:[]},{addItemButtonLabel:"Add Stop Sequences",componentName:t.componentName.repeaterField,fieldsMetaData:[{componentName:t.componentName.input,label:e("Stop Sequences"),name:"stop-sequences-id",required:!0,value:void 0}],helperText:e("Maximum of 4 sequences that will trigger OpenAI to stop generating further text. The returned text will not contain the stop sequences."),id:"stop-sequences-list",inputGroupProps:{label:e("Stop Sequences #{COUNT}")},label:e("Stop Sequences"),loading:!1,onChange:"ON_FIELD_MAP_CHANGE",render:"IF_ADVANCE_FEATURE_SELECTED",value:[]},{addItemButtonLabel:e("Add Optional Fields"),componentName:t.componentName.repeaterField,direction:"horizontal",fieldsMetaData:[{componentName:t.componentName.select,id:"optional-fields",label:e("Field"),name:"optional-fields",options:T,required:!0,style:{width:150},value:""},{componentName:t.componentName.mixInput,label:e("Value"),name:"value",required:!0,value:[],wrapperClassName:"w-100"}],id:"optional-field-list",inputGroupProps:{label:e("Field #{COUNT}")},label:e("Optional Fields"),onChange:"ON_OPTIONAL_FIELD_MAP",path:"optionalFields",render:"IF_ADVANCE_FEATURE_SELECTED",value:[]}]},actions:{SET_CONNECTION:async({$:a,e:o})=>{a.setThisComponent(n=>{n.value=o}),a.setDb(n=>{n.connectionId=o}),await u(a)},IF_CONNECTION_SELECTED:({$:a})=>{var o;return r((o=a.getComponent("connection-id"))==null?void 0:o.value)},IF_REASONING_MODEL_SELECTED:({$:a})=>{var m,p,d;const o=(m=a.getComponent("model-id"))==null?void 0:m.value,n=((p=a.getComponent("show-advance-feature"))==null?void 0:p.value)==="true";if(r((d=a.getComponent("connection-id"))==null?void 0:d.value)&&n&&["o3-mini","o3-mini-2025-01-31","o1","o1-2024-12-17"].includes(o))return!0},IF_MODEL_SELECTED:({$:a})=>{var o,n;if(r((o=a.getComponent("connection-id"))==null?void 0:o.value)&&((n=a.getComponent("model-id"))==null?void 0:n.value)!=="")return!0},IF_ADVANCE_FEATURE_SELECTED:({$:a})=>{var o,n;if(r((o=a.getComponent("connection-id"))==null?void 0:o.value)&&((n=a.getComponent("show-advance-feature"))==null?void 0:n.value)===!0)return!0},REFETCH_MODEL_LIST:async({$:a})=>{await u(a)},SET_MODEL:({$:a,e:o})=>{a.setThisComponent(i=>{i.value=o}),a.setDb(i=>{i.model=o});const n=["o3-mini","o3-mini-2025-01-31","o1","o1-2024-12-17"],s=["o1-preview-2024-09-12","o4-mini-2025-04-16","o1-preview","o1-2024-12-17","o1","o1-mini","o4-mini","o1-mini-2024-09-12"];n.includes(o)&&a.setComponent("reasoning_effort",i=>{a.util.isSelectComponent(i)&&(i.value=void 0)}),s.includes(o)&&a.setComponent("max-token",i=>{a.util.isMixInputComponent(i)&&(i.path="max_completion_tokens")})},SET_MAX_TOKEN:l("maxToken"),ON_FIELD_MAP_CHANGE:l("messagesList"),ON_OPTIONAL_FIELD_MAP:l("optionalFields"),SET_FEATURE:({$:a,e:o})=>{a.setThisComponent(n=>{n.value=o}),a.setDb(n=>{n.advanceFeature=o}),o===!1&&v.map(n=>a.setComponent(n,s=>{s.value=void 0}))},SET_RESPONSE:l("responseId"),SET_REASONING_EFFECT:l("reasoningEffect"),SET_TEMPERATURE:l("temperatureId"),SET_TOP_P:l("topPId"),SET_NUMBER:l("numberId"),SET_FREQUENCY_PENALTY:l("frequencyId"),SET_PRESENCE_PENALTY:l("presenceId"),SET_SEED:l("seedId"),CONNECTION_ADD_CHANGE:_,ON_MACHINE_LOAD:async({$:a})=>{var o;E(a,[{db:"connectionId",id:"connection-id"},{db:"model",id:"model-id"},{db:"maxToken",id:"max-token"},{db:"messagesList",id:"messages-list"},{db:"advanceFeature",id:"show-advance-feature"},{db:"reasoningEffect",id:"reasoning_effort"},{db:"temperatureId",id:"Temperature"},{db:"optionalFields",id:"optional-field-list"}]),(o=a.db)!=null&&o.connectionId&&await u(a)}}}));export{I as default};
