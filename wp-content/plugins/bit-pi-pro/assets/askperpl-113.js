import{a3 as l,a4 as r}from"./_applist-952.js";import{_ as e}from"./main-red-poems-invite.js";import{s as i}from"./machine.-578.js";import{h as n,a as p,c as s}from"./commonut-587.js";import"./lodash-840.js";import"./mutative-12.js";globalThis.jotaiAtomCache=globalThis.jotaiAtomCache||{cache:new Map,get(t,a){return this.cache.has(t)?this.cache.get(t):(this.cache.set(t,a),a)}};const _=l(({helpers:t})=>({states:{components:[s(t),{componentName:t.componentName.select,helperText:e("Choose the perplexity model to use here."),id:"model",label:e("Model"),onChange:"SELECT_MODEL",optionFilterProp:"label",options:[{label:e("sonar"),value:"sonar"},{label:e("sonar-pro"),value:"sonar-pro"},{label:e("sonar-reasoning"),value:"sonar-reasoning"},{label:e("sonar-deep-research"),value:"sonar-deep-research"},{label:e("sonar-reasoning-pro"),value:"sonar-reasoning-pro"},{label:e("r1-1776"),value:"r1-1776"}],path:"model",placeholder:e("Select a Model"),render:"IF_CONNECTION_SELECTED",showSearch:!0,value:void 0},{componentName:t.componentName.mixInput,helperText:e("Write the message"),id:"message",label:e("Message"),onChange:"HANDLE_MESSAGE",path:"message",placeholder:e("Enter message"),render:"IF_CONNECTION_SELECTED",value:void 0},{componentName:t.componentName.switch,fieldType:"config",id:"show-advance-feature",label:e("Show Advance Feature"),onChange:"SHOW_FEATURE",render:"IF_CONNECTION_SELECTED",value:!1},{componentName:t.componentName.mixInput,helperText:e("Enter the maximum number of completion tokens. Ensure the total of max_tokens and prompt tokens does not exceed the model's context window limit. If left blank, the model will generate tokens until a stop token or the context window end is reached. E.g. 900."),id:"max-token",label:e("Max Token"),onChange:"SET_MAX_TOKEN",path:"max_tokens",placeholder:e("Enter the token value"),render:"IF_ADVANCE_FEATURE_ENABLED",value:void 0},{componentName:t.componentName.mixInput,helperText:e("Enter the temperature value here, which must be between 0 (inclusive) and 1.999 (inclusive). This controls the randomness of the response: higher values introduce more randomness, while lower values result in more deterministic outputs. E.g. 1.5."),id:"temperature-val",label:e("Temperatures"),onChange:"SET_TEMPERATURE",path:"temperature",placeholder:e("Enter temperature value"),render:"IF_ADVANCE_FEATURE_ENABLED",value:void 0},{componentName:t.componentName.mixInput,helperText:e("Enter a value between -2.0 and 2.0. Positive values penalize new tokens based on their occurrence in the text, encouraging the model to introduce new topics. This setting is incompatible with the frequency_penalty. E.g. 1."),id:"presence_penalty",label:e("Presence Penalty"),onChange:"SET_PRESENCE_PENALTY",path:"presence_penalty",placeholder:e("Enter presence-penalty value"),render:"IF_ADVANCE_FEATURE_ENABLED",value:void 0},{componentName:t.componentName.mixInput,helperText:e("Enter a multiplicative penalty greater than 0. Values above 1.0 reduce the likelihood of repeating tokens, with 1.0 indicating no penalty. This setting is incompatible with the presence_penalty. E.g. 1."),id:"frequency_penalty",label:e("Frequency Penalty"),onChange:"SET_FREQUENCY_PENALTY",path:"frequency_penalty",placeholder:e("Enter frequency-penalty value"),render:"IF_ADVANCE_FEATURE_ENABLED",value:void 0},{componentName:t.componentName.mixInput,helperText:e("Enter the nucleus sampling threshold, which should be a value between 0 and 1 (inclusive). For each subsequent token, the model will consider the results of tokens within the top_p probability mass. We recommend adjusting either the top_k or top_p parameter, but not both simultaneously. E.g. 0.7."),id:"topP-val",label:e("topP Sampling Value"),onChange:"SET_TOP_P",path:"top_p",placeholder:e("Enter topP value"),render:"IF_ADVANCE_FEATURE_ENABLED",value:void 0},{componentName:t.componentName.mixInput,helperText:e("Enter the number of tokens to retain for top-k filtering, using an integer between 0 and 2048 (inclusive). Setting this value to 0 will disable top-k filtering. We recommend adjusting either the top_k or top_p parameter, but not both simultaneously. E.g. 1000."),id:"topK-val",label:e("topK Sampling Value"),onChange:"SET_TOP_K",path:"top_k",placeholder:e("Enter topK value"),render:"IF_ADVANCE_FEATURE_ENABLED",value:void 0}]},actions:{IF_ADVANCE_FEATURE_ENABLED:({$:a})=>{var o;return((o=a.getComponent("show-advance-feature"))==null?void 0:o.value)===!0},IF_CONNECTION_SELECTED:({$:a})=>{var o;return r((o=a.getComponent("connection-id"))==null?void 0:o.value)},ON_MACHINE_LOAD:async({$:a})=>{i(a,[{db:"connectionId",id:"connection-id"},{db:"modelDb",id:"model"},{db:"maxToken",id:"max-token"},{db:"temperatureVal",id:"temperature-val"},{db:"topPVal",id:"topP-val"},{db:"topKVal",id:"topK-val"},{db:"showAdvanceFeature",id:"show-advance-feature"},{db:"messageDb",id:"message"},{db:"frequencyPenalty",id:"frequency_penalty"},{db:"presencePenalty",id:"presence_penalty"}])},CONNECTION_ADD_CHANGE:p,HANDLE_MESSAGE:n("messageDb"),SELECT_MODEL:n("modelDb"),SET_CONNECTION:n("connectionId"),SET_FREQUENCY_PENALTY:n("frequencyPenalty"),SET_MAX_TOKEN:n("maxToken"),SET_PRESENCE_PENALTY:n("presencePenalty"),SET_TEMPERATURE:n("temperatureVal"),SET_TOP_K:n("topKVal"),SET_TOP_P:n("topPVal"),SHOW_FEATURE:n("showAdvanceFeature")}}));export{_ as default};
